# @Author: zechenghe
# @Date:   2019-01-21T12:01:09-05:00
# @Last modified by:   zechenghe
# @Last modified time: 2019-02-01T14:50:41-05:00

import time
import math
import os
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image

import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.backends.cudnn as cudnn


def clip_grad(parameters, grad_norm_bound):
    # """Clips gradient norm of an iterable of parameters.

    # The norm is computed over all gradients together, as if they were
    # concatenated into a single vector. Gradients are modified in-place.
    


    # Arguments:
    #     parameters (Iterable[Tensor]): an iterable of Tensors that will have gradients normalized
    #     grad_norm_bound (float or int): max norm of the gradients

    # Returns:
    #     Total norm of the parameters (viewed as a single vector).
    # """
 
### get gradient
    if isinstance(parameters, torch.Tensor):
        parameters = [parameters]

    parameters = list(filter(lambda p: p.grad is not None, parameters))
    C = float(grad_norm_bound)                   # gradient norm bound
    norm_type = 2.0                        # norm type

    total_norm = 0
    hehe = []
    for p in parameters:
        param_norm = p.grad.data.norm(norm_type)            # norm2 of p
        hehe.append(param_norm.item())

        total_norm += param_norm.item() ** norm_type              #
    
    # min_p = []
    # max_p = []
    # for i in range(len(parameters)):
    #     min_p.append(torch.min(parameters[i]))
    #     max_p.append(torch.max(parameters[i]))
    
    
    total_norm = total_norm ** (1. / norm_type)
    hehe = np.array(hehe)
    
    

    clip_coef = C / (total_norm + 1e-6)
    

    if clip_coef < 1:   
        for p in parameters:
            p.grad.data.mul_(clip_coef)
    return total_norm


def add_noise(parameters, grad_norm_bound, scale):
    # """Add noise to gradiens of an iterable of parameters.

    # The noise is generated by pytorch build-in function.
    # Gradients are modified in-place.

    # Arguments:
    #     parameters (Iterable[Tensor]): an iterable of Tensors that will have gradients normalized
    #     grad_norm_bound (float or int): max norm of the gradients
    #     scale (float or int): noise scale
    #     device (str): cpu or cuda

    # Returns:
    #     None.
    # """
    if isinstance(parameters, torch.Tensor):
        parameters = [parameters]

    parameters = list(filter(lambda p: p.grad is not None, parameters))
    # C = float(grad_norm_bound)
    
    scale = float(scale)
    
    generator = torch.distributions.normal.Normal(torch.tensor(0.0),scale)
    
    
    # scale = float(scale) * C    # NOT SURE!!!
    # # https://pytorch.org/docs/stable/_modules/torch/distributions/laplace.html
    
    # generator = torch.distributions.Laplace(0.0, scale)
    
    for p in parameters:
        grad_shape = p.grad.data.shape
        noise = generator.sample(grad_shape) # generate noise with shape "grad_shape"
        noise = noise.cuda()
        p.grad.data.add_(noise)              # add noise to grad


def add_noise_inference(parameters, scale):
 
    # if isinstance(parameters, torch.Tensor):
    #     parameters = [parameters]

    # parameters = list(filter(lambda p: p.grad is not None, parameters))
    # C = float(grad_norm_bound)
    # param = []
    # for p in parameters:
    #     param.append(p)
    
    scale = float(scale)
    
    generator = torch.distributions.normal.Normal(torch.tensor(0.0),scale)
    
    
    # scale = float(scale) * C    # NOT SURE!!!
    # # https://pytorch.org/docs/stable/_modules/torch/distributions/laplace.html
    
    # generator = torch.distributions.Laplace(0.0, scale)
    
    for p in parameters:
        
        print torch.max(p)
        grad_shape = p.data.shape
        noise = generator.sample(grad_shape) # generate noise with shape "grad_shape"
        noise = noise.cuda()
        p.data.add_(noise)              # add noise to grad
        print torch.max(p)


def accuracy(predictions, labels):

    if not (predictions.shape == labels.shape):
        print "predictions.shape ", predictions.shape, "labels.shape ", labels.shape
        raise AssertionError

    correctly_predicted = np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
    accu = (100.0 * correctly_predicted) / predictions.shape[0]
    return accu

def pseudoInverse(W):
    return np.linalg.pinv(W)


def getImgByClass(X_test, Y_test, C):
    
    fff = [0,1]

    
    batchx = X_test[fff[C]]        
    batchy = Y_test[fff[C]]

    return batchx, batchy    



def clip(data):
    data[data > 1.0] = 1.0
    data[data < 0.0] = 0.0
    return data

# def preprocess(data):

#     size = data.shape
#     NChannels = size[-1]
#     assert NChannels == 1 or NChannels == 3
#     if NChannels == 1:
#         mu = 0.5
#         sigma = 0.5
#     elif NChannels == 3:
#         mu = [0.485, 0.456, 0.406]
#         sigma = [0.229, 0.224, 0.225]
#     data = (data - mu) / sigma

#     assert data.shape == size
#     return data

def preprocess(data):

    mu = 105.925
    sigma = 58.564  
   

    data = data.astype(np.float64)
    data = (data - mu) / sigma

    return data

# def deprocess(data):


#     mu = torch.tensor([208.21], dtype=torch.float32)
#     sigma = torch.tensor([43.025], dtype=torch.float32)


#     Unnormalize = transforms.Normalize((-mu / sigma).tolist(), (1.0 / sigma).tolist())
#     return clip(Unnormalize(data[0,:,:,:]).unsqueeze(0)) 

def deprocess(data):


    mu = 105.925
    sigma = 58.564  
    
    data = data * sigma + mu
    # data = data * 255

    return data

def evalTest(X_test, Y_test, net, gpu = True):

    acc = 0.0
    NBatch = 0
    predd = 0
    for i in range(len(X_test)):
        batchx = X_test[i]
        batchx = torch.from_numpy(batchx)
        batchx = batchx.reshape(-1,1,64,64).type(torch.FloatTensor)
        # batchx = batchx.reshape(-1,1,1024,1024).type(torch.FloatTensor)
            
        batchy = Y_test[i]
        batchy = torch.Tensor([batchy]).long()

        NBatch += 1

        if gpu:
            batchx = batchx.cuda()
            batchy = batchy.cuda()
        logits = net.forward(batchx)

        if gpu:
            pred = np.argmax(logits.cpu().detach().numpy(), axis = 1)
            groundTruth = batchy.cpu().detach().numpy()
        else:
            pred = np.argmax(logits.detach().numpy(), axis = 1)
            groundTruth = batchy.detach().numpy()
        predd += np.sum(pred == 0)
        acc += np.sum(pred == groundTruth)
    accTest = acc / NBatch
    
    print "Label0 ", predd
    print "Test accuracy: ", accTest #, "NBatch: ", NBatch, "pred == groundTruth.shape", (pred == groundTruth).shape
    return accTest



def weight_init(m):
    '''
    Usage:
        model = Model()
        model.apply(weight_init)
    '''
    if isinstance(m, nn.Conv1d):
        init.normal_(m.weight.data)
        if m.bias is not None:
            init.normal_(m.bias.data)
    elif isinstance(m, nn.Conv2d):
        init.xavier_normal_(m.weight.data)
        if m.bias is not None:
            init.normal_(m.bias.data)
    elif isinstance(m, nn.Conv3d):
        init.xavier_normal_(m.weight.data)
        if m.bias is not None:
            init.normal_(m.bias.data)
    elif isinstance(m, nn.ConvTranspose1d):
        init.normal_(m.weight.data)
        if m.bias is not None:
            init.normal_(m.bias.data)
    elif isinstance(m, nn.ConvTranspose2d):
        init.xavier_normal_(m.weight.data)
        if m.bias is not None:
            init.normal_(m.bias.data)
    elif isinstance(m, nn.ConvTranspose3d):
        init.xavier_normal_(m.weight.data)
        if m.bias is not None:
            init.normal_(m.bias.data)
    elif isinstance(m, nn.BatchNorm1d):
        init.normal_(m.weight.data, mean=1, std=0.02)
        init.constant_(m.bias.data, 0)
    elif isinstance(m, nn.BatchNorm2d):
        init.normal_(m.weight.data, mean=1, std=0.02)
        init.constant_(m.bias.data, 0)
    elif isinstance(m, nn.BatchNorm3d):
        init.normal_(m.weight.data, mean=1, std=0.02)
        init.constant_(m.bias.data, 0)
    elif isinstance(m, nn.Linear):
        init.xavier_normal_(m.weight.data)
        init.normal_(m.bias.data)
    elif isinstance(m, nn.LSTM):
        for param in m.parameters():
            if len(param.shape) >= 2:
                init.orthogonal_(param.data)
            else:
                init.normal_(param.data)
    elif isinstance(m, nn.LSTMCell):
        for param in m.parameters():
            if len(param.shape) >= 2:
                init.orthogonal_(param.data)
            else:
                init.normal_(param.data)
    elif isinstance(m, nn.GRU):
        for param in m.parameters():
            if len(param.shape) >= 2:
                init.orthogonal_(param.data)
            else:
                init.normal_(param.data)
    elif isinstance(m, nn.GRUCell):
        for param in m.parameters():
            if len(param.shape) >= 2:
                init.orthogonal_(param.data)
            else:
                init.normal_(param.data)

def setLearningRate(optimizer, lr):
    """Sets the learning rate to the initial LR decayed by 10 every 30 epochs"""
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr


def TV(x):
    batch_size = x.size()[0]
    h_x = x.size()[2]
    w_x = x.size()[3]
    count_h = _tensor_size(x[:,:,1:,:])
    count_w = _tensor_size(x[:,:,:,1:])
    h_tv = torch.pow(x[:,:,1:,:]-x[:,:,:h_x-1,:], 2).sum()
    w_tv = torch.pow(x[:,:,:,1:]-x[:,:,:,:w_x-1], 2).sum()
    return (h_tv / count_h + w_tv / count_w) / batch_size

def _tensor_size(t):
    return t.size()[1]*t.size()[2]*t.size()[3]


def l2loss(x):
    return (x**2).mean()

def l1loss(x):
    return (torch.abs(x)).mean()

def getL1Stat(net, x):
    for layer in net.layerDict:
        targetLayer = net.layerDict[layer]
        layerOutput = net.getLayerOutput(x, targetLayer)
        print "Layer " + layer + ' l1 loss:', l1loss(layerOutput).cpu().detach().numpy()


def getModule(net, blob):
    modules = blob.split('.')
#    print "Target layer: ", modules
#    if len(modules) == 1:
#        return net._modules.get(blob)
#    else:

    curr_module = net
    print curr_module
    for m in modules:
        curr_module = curr_module._modules.get(m)
    return curr_module

def getLayerOutputHook(module, input, output):
    if not hasattr(module, 'activations'):
        module.activations = []
    module.activations.append(output)

def getHookActs(model, module, input):
    if hasattr(module, 'activations'):
        del module.activations[:]
    _ = model.forward(input)
    assert(len(module.activations) == 1)
    return module.activations[0]

def saveImage(img, filepath):
    torchvision.utils.save_image(img, filepath)


def evalTestSplitModel(testloader, netEdge, netCloud, layer, gpu):
    testIter = iter(testloader)
    acc = 0.0
    NBatch = 0
    for i, data in enumerate(testIter, 0):
        batchX, batchY = data
        if gpu:
            batchX = batchX.cuda()
            batchY = batchY.cuda()

        try:
            edgeOutput = netEdge.getLayerOutput(batchX, netEdge.layerDict[layer]).clone()
        except Exception, e:
            #print "Except in evalTestSplitModel getLayerOutput, this is a Edge-only model"
            #print str(e)
            edgeOutput = netEdge.forward(batchX).clone()

        #cloudOuput = net.forward(batchX)
        logits = netCloud.forward_from(edgeOutput, layer)

        #softmax = nn.Softmax().cuda()
        #prob = softmax(logits)
        #print prob[:100,:].max(dim=1)

        if gpu:
            pred = np.argmax(logits.cpu().detach().numpy(), axis = 1)
            groundTruth = batchY.cpu().detach().numpy()
        else:
            pred = np.argmax(logits.detach().numpy(), axis = 1)
            groundTruth = batchY.detach().numpy()
        acc += np.mean(pred == groundTruth)
        NBatch += 1

    accTest = acc / NBatch
    #print "Test accuracy: ", accTest #, "NBatch: ", NBatch, "pred == groundTruth.shape", (pred == groundTruth).shape
    return accTest

def get_PSNR(refimg, invimg, peak = 1.0):
    psnr = 10*np.log10(peak**2 / np.mean((refimg - invimg)**2))
    return psnr
